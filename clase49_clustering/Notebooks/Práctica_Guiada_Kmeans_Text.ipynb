{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica Guiada II - Kmeans Clustering y reducción de dimensiones sobre texto\n",
    "\n",
    "En esta práctica guiada vamos a aplicar el algoritmo kmeans a textos de distintos tópicos para verificar si el agrupamiento coincide con las categorías que se les dieron a los mismos.\n",
    "\n",
    "* Primero vamos a pasar de los features de texto a un bag of words.\n",
    "* Luego vamos reducir la dimensionadlidad del bag of words con una técnica de reducción especial para trabajar con espacios súper-dispersos llamada <a href='http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html'>Truncated SVD </a>\n",
    "* Por último vamos a aplicar Kmeans con tantos clusters como clases había inicialmente para luego visualizar los datos y calcular métricas de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importamos librerías\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Construcción del dataset\n",
    "\n",
    "Importamos el dataset newsgroup20 con emails clasificados en distintas categorías. Seleccionamos específicamente 3 de las categorías existentes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando categorías:\n",
      "['talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "2588 documentos\n",
      "3 categorías\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################\n",
    "# Cargamos las categorías\n",
    "categories = [\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "print(\"Cargando categorías:\")\n",
    "print(categories)\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "print(\"%d documentos\" % len(dataset.data))\n",
    "print(\"%d categorías\" % len(dataset.target_names))\n",
    "print()\n",
    "\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Vectorización y Bag of Words\n",
    "\n",
    "A continuación construimos el Bag of Words, limitando la ocurrencia de las palabras con max-df y min-df, deshaciéndonos de las stop words y con un máximo de 10000 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2588, n_features: 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                             min_df=2, stop_words='english',\n",
    "                             use_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Reducción de dimensiones\n",
    "\n",
    "Para la reducción de dimensiones usamos TruncatedSVD(). Los resultados de este algoritmo no están normalizados, y para el buen funcionamiento de kmeans es importante que todos los features se encuentren en la misma escala, por ser un algoritmo basado en la distancia.\n",
    "El método Normalizer() de sklearn reescala todos los features para tener norma l2 igual a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(100,random_state = 1)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "X = lsa.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Clustering\n",
    "\n",
    "Con este preprocesamiento, estamos listos para aplicar kmeans. Vamos a utilizar 3 centroides para ver si podemos detectar la estructura de las categorías originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=3, n_init=1, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=1, tol=0.0001, verbose=1)\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 2310.24120137\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 2256.39380889\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 2242.86662115\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 2241.0565545\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 2240.64812454\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 2240.06951145\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 2238.31032003\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 2236.70447913\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 2235.42778694\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 2234.04367176\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 2233.76094434\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 11, inertia 2233.65984874\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 12, inertia 2233.62765\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 13, inertia 2233.61526751\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 14, inertia 2233.61202759\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 15, inertia 2233.60911095\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 16, inertia 2233.60911095\n",
      "center shift 0.000000e+00 within tolerance 9.121018e-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "            verbose=1, random_state = 1)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "km.fit(X)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Validación de resultados\n",
    "\n",
    "Lo primero que vamos a hacer para explorar los resultados es volver atrás en la reducción de dimensiones para verificar cuáles son las palabras que con más fuerza componen cada uno de los clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking de términos por cluster:\n",
      "Cluster 0: space nasa access henry com digex pat gov toronto shuttle\n",
      "Cluster 1: god com jesus people christian sandvik don bible article say\n",
      "Cluster 2: graphics university thanks image com files help 3d posting computer\n"
     ]
    }
   ],
   "source": [
    "print(\"Ranking de términos por cluster:\")\n",
    "\n",
    "# Los centroides están mapeados en el espacio de dimeniosnes reducidas.\n",
    "# Hacemos la transformación inversa para ver el peso de los features originales.\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "\n",
    "# La función argsort nos da los índices ordenados\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "\n",
    "\n",
    "# Imprimimos los términos más \"pesados\" (frecuentes) para cada categoría.\n",
    "# Con los índices ordenados, llamamos a la descripción de las palabras que construyó el Vectorizer()\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    \n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.040026946088605443"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X, km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que un Silhouette Score cercano a 0 indica que hay clusters que se superponen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medidas de validación externa\n",
    "\n",
    "En base a los términos encontrados en los clusters podríamos concluir que 0 es 'sci.space', el 1 'talk.religion' y el 2 es 'comp.graphics'.\n",
    "Con esto podemos calcular el cluster predicho para cada noticia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predY = np.choose(dataset.target, [2, 0, 1]).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[919,   7,  61],\n",
       "       [ 15, 598,  15],\n",
       "       [ 18,  21, 934]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix(predY,km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94706336939721791"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predY,km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
