{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REPASO DE CLASIFICACIÓN\n",
    "## Flujo de trabajo con pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generaremos un flujo de trabajo para abordar un problema de clasificación a partir del dataset de cancer de mama de Wisconsin, ya conocido. Aquí veremos Regresión Logística y SVM, luego pueden adicionar NB Gausiano y KNN al análisis/repaso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases'\n",
    "                 '/breast-cancer-wisconsin/wdbc.data', header=None)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos la variable target a numérica\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df.loc[:, 2:].values\n",
    "y = df.loc[:, 1].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generamos el split de nuestro dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.30, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementamos un pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# PIPELINE: encapsulamos los procesos de estandarización de features, reducción de dimensionalidad y modelado\n",
    "\n",
    "pipe_lr = make_pipeline(StandardScaler(),\n",
    "                        PCA(n_components=2),\n",
    "                        LogisticRegression(random_state=1))\n",
    "\n",
    "# Fiteamos el modelo\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "# Implementamos el modelo para luego evaluarlo\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "\n",
    "print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera instancia, el modelo da muy bien... Aunque aún no hemos incluido cross validation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validación del modelo con K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "# Generamos el mecanismo de partición del train sest\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=1, shuffle=True).split(X_train, y_train)\n",
    "\n",
    "# Iteramos sobre cada partición y registramos scores\n",
    "\n",
    "scores = []\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    \n",
    "    pipe_lr.fit(X_train[train], y_train[train])\n",
    "    score = pipe_lr.score(X_train[test], y_train[test])\n",
    "    scores.append(score)\n",
    "    \n",
    "    print('Fold: %2d, Class dist.: %s, Acc: %.3f' % \\\n",
    "          (k+1,\n",
    "           # np.bincount cuenta la ocurrencia de cada clase en los splits. Notar que cada split está balanceado.\n",
    "           np.bincount(y_train[train]), \n",
    "           score))\n",
    "    \n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Generamos CV equivalente o alternativo\n",
    "\n",
    "scores = cross_val_score(estimator=pipe_lr,\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=5,\n",
    "                         n_jobs=1)\n",
    "\n",
    "print('CV accuracy: %s' % scores)\n",
    "print('Resumen CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Imprimimos la matriz de confusión\n",
    "\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curvas de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`learning_curve` itera las particiones del dataset en train y test hasta encontrar la cantidad óptima de samples, es decir, el punto donde se estabiliza el score sobre el test por más que incorporemos un mayor número de samples al train set. De esta forma, podemos eliminar el over-fitting garantizando el mínimo sesgo. Documentación [aquí](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Modificamos nuestro PIPELINE original\n",
    "# Quitamos PCA e incluimos penalización L2\n",
    "pipe_lr = make_pipeline(StandardScaler(),\n",
    "                        LogisticRegression(penalty='l2', random_state=1))\n",
    "\n",
    "# Implementamos una learning_curve\n",
    "train_sizes, train_scores, test_scores =\\\n",
    "                learning_curve(estimator=pipe_lr,\n",
    "                               X=X_train,\n",
    "                               y=y_train,\n",
    "                               train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                               cv=10,\n",
    "                               n_jobs=1)\n",
    "\n",
    "# Calculamos una medida resumen de las estimadores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos\n",
    "plt.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.8, 1.03])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curva de validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`validation_curve` permite evaluar la performance de nuestro modelo (el pipeline en este caso) con distintos valores del hiperparámetro asociado, es una alternativa para lidiar con el over-fitting y evitar el under-fitting o una mala especificación del modelo. Documentación [aquí](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Generamos valores para el hiperparámetro C de la regresión logística\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "# Implementamos una curva de validación\n",
    "train_scores, test_scores = validation_curve(\n",
    "                estimator=pipe_lr, \n",
    "                X=X_train, \n",
    "                y=y_train, \n",
    "                param_name='logisticregression__C', \n",
    "                param_range=param_range,\n",
    "                cv=10)\n",
    "\n",
    "# Calculamos una medida resumen de las estimadores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Graficamos\n",
    "plt.plot(param_range, train_mean, \n",
    "         color='blue', marker='o', \n",
    "         markersize=5, label='training accuracy')\n",
    "\n",
    "plt.fill_between(param_range, train_mean + train_std,\n",
    "                 train_mean - train_std, alpha=0.15,\n",
    "                 color='blue')\n",
    "\n",
    "plt.plot(param_range, test_mean, \n",
    "         color='green', linestyle='--', \n",
    "         marker='s', markersize=5, \n",
    "         label='validation accuracy')\n",
    "\n",
    "plt.fill_between(param_range, \n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std, \n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/06_06.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "## Tuneo de hiperparámetros con GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto, GridSearch es una alternativa para determinar cuál es el mejor hiperparámetro (o combinaciones de hiperparámetros y/o kernels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generamos el pipeline para SVM\n",
    "pipe_svc = make_pipeline(StandardScaler(),\n",
    "                         SVC(random_state=1))\n",
    "\n",
    "# Generamos valores para los hiperparámetros C y gamma\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "# Generamos el grid de parámetros y kernel correspondiente\n",
    "param_grid = [{'svc__C': param_range, \n",
    "               'svc__kernel': ['linear']},\n",
    "              {'svc__C': param_range, \n",
    "               'svc__gamma': param_range, \n",
    "               'svc__kernel': ['rbf']}]\n",
    "\n",
    "# Implementamos GridSearch\n",
    "gs = GridSearchCV(estimator=pipe_svc, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos los mejores estimadores\n",
    "svm = gs.best_estimator_\n",
    "svm.fit(X_train, y_train)\n",
    "print('Test accuracy: %.3f' % svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos la matriz de confusión\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secuencia de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=pipe_svc,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=2)\n",
    "\n",
    "scores = cross_val_score(gs, X_train, y_train, \n",
    "                         scoring='accuracy', cv=5)\n",
    "\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores),\n",
    "                                      np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medidas de evaluación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusión\n",
    "\n",
    "Evaluamos la matriz de confusión de SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "y_pred = pipe_svc.predict(X_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/06_09.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué nos está diciendo el modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una situación común es que querramos que el \"best estimator\" de cross validation sea una métrica en particular, por ejemplo, el f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Implementamos make_scorer para evaluar con GridSearch\n",
    "scorer = make_scorer(f1_score, pos_label=0)\n",
    "\n",
    "c_gamma_range = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "param_grid = [{'svc__C': c_gamma_range,\n",
    "               'svc__kernel': ['linear']},\n",
    "              {'svc__C': c_gamma_range,\n",
    "               'svc__gamma': c_gamma_range,\n",
    "               'svc__kernel': ['rbf']}]\n",
    "\n",
    "# Incluimos el scorer en el GridSearch\n",
    "gs = GridSearchCV(estimator=pipe_svc,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring=scorer,\n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve y AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "# Cambiamos nuestro pipeline de LR\n",
    "pipe_lr = make_pipeline(StandardScaler(),\n",
    "                        PCA(n_components=2),\n",
    "                        LogisticRegression(penalty='l2', \n",
    "                                           random_state=1, \n",
    "                                           C=100.0))\n",
    "\n",
    "X_train2 = X_train[:, [4, 14]]\n",
    "\n",
    "# Instrumentamos CV\n",
    "cv = list(StratifiedKFold(n_splits=3, \n",
    "                          random_state=1).split(X_train, y_train))\n",
    "\n",
    "# Graficamos\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "all_tpr = []\n",
    "\n",
    "# Iteramos el pipeline sobre CV, calculando probabilidades de la predicción y la curva ROC\n",
    "for i, (train, test) in enumerate(cv):\n",
    "    probas = pipe_lr.fit(X_train2[train],\n",
    "                         y_train[train]).predict_proba(X_train2[test])\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_train[test],\n",
    "                                     probas[:, 1],\n",
    "                                     pos_label=1)\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr,\n",
    "             tpr,\n",
    "             label='ROC fold %d (area = %0.2f)'\n",
    "                   % (i+1, roc_auc))\n",
    "\n",
    "plt.plot([0, 1],\n",
    "         [0, 1],\n",
    "         linestyle='--',\n",
    "         color=(0.6, 0.6, 0.6),\n",
    "         label='random guessing')\n",
    "\n",
    "mean_tpr /= len(cv)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--',\n",
    "         label='mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "plt.plot([0, 0, 1],\n",
    "         [0, 1, 1],\n",
    "         linestyle=':',\n",
    "         color='black',\n",
    "         label='perfect performance')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/06_10.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
