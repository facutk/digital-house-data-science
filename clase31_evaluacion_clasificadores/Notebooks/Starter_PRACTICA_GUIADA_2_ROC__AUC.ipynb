{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA GUIADA : Curva ROC y Area bajo la curva\n",
    "\n",
    "## 1. Introducción\n",
    "\n",
    "Seguimos trabajando sobre el dataset de RRHH. La variable dependiente es la misma ($P(left=1|X)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_curve, auc, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/HR_comma_sep.csv')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cols = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', \n",
    "              'time_spend_company', 'Work_accident', 'promotion_last_5years']\n",
    "X = df[train_cols]\n",
    "y = df['left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Definimos y entrenamos el modelo (Regresión Logística)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1e10)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ajustando los umbrales\n",
    "\n",
    "Hasta ahora siempre hemos trabajado asumiendo que si $p(y=1) > 0.5$, entonces, la predicción del modelo será que $y=1$. Ahora bien, ¿qué sucede si queremos maximizar o modificar la performance del modelo en alguna de las métricas que hemos visto antes (sentivity, recall, etc.)? ¿Cómo podemos lograr esto?\n",
    "\n",
    "Una forma es haciendo variar esa regla que habíamos definido más arriba: ajustar los umbrales.\n",
    "\n",
    "Veamos cómo funciona. En primer lugar, obtengamos las predicciones de probabilidad ($p(y=1)$) y no las predicciones de la clase de $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que el método `predict_proba` nos devuelve una array en el cual aparecen dos probabilidades de cada instancia del test set: $p(y=0)$ y $p(y=1)$, en ese orden.\n",
    "\n",
    "`sklearn` realiza la predicción de la clase de $y$ eligiendo para cada clase la mayor probabilidad de este array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(clf.predict_proba(X_test)[:,1] > 0.5))\n",
    "print(np.mean(clf.predict(X_test)))\n",
    "\n",
    "y_pred_orig = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardemos las probabilidades de ambas clases en un array y $p(y=1)$ en otro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_probs_logit = clf.predict_proba(X_test)\n",
    "y_probs_logit_left = y_probs_logit[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 bins\n",
    "plt.hist(y_probs_logit_left, bins=15)\n",
    "\n",
    "# x-axis de 0 a 1\n",
    "plt.xlim(0,1)\n",
    "plt.title('Histograma de probabilidades estimadas')\n",
    "plt.xlabel('Probabilidad estimada de dejar la empresa')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La gran mayoría de las probabilidades predichas van de 0.0 a 0.4\n",
    "* Hay un escaso número de probabilidades estimadas mayores a 0.5\n",
    "* La mayor parte de los casos van a ser predichos como $y=0$ es decir, que no se van de la empresa.\n",
    "\n",
    "Una posible solución es, entonces, variar el umbral -$y = 1$ si $p(y=1) > 0.5$-. \n",
    "\n",
    "En este caso, lo lógico sería bajar el umbral. Por lo tanto se incrementará la sensitivity. ¿Por qué?\n",
    "\n",
    "* Aumentaremos la cantidad de TP\n",
    "* El clasificador será más \"sensible\" a las instancias posiitvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "y_pred_logit = binarize(y_probs_logit, 0.3)[:,1]\n",
    "\n",
    "# Otra forma\n",
    "#y_pred_logit = (y_probs_logit_left > 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Veamos la matriz de confusión con las predicciones basadas en el modelo original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Veamos la matriz de confusión con las predicciones basadas en el modelo modificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Qué pasó con la sensitividad (o recall)? \n",
    "\n",
    "$\\large recall = \\frac{TP}{(FN + TP)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Recall umbral 0.5=', recall_score(y_test, y_pred_orig))\n",
    "print('Recall umbral 0.3=', recall_score(y_test, y_pred_logit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Qué pasó con la specificity? \n",
    "\n",
    "$\\large specificity = \\frac{TN}{(TN + FP)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def specificy(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    return(specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spec umbral 0.5 =', specificy(y_test, y_pred_orig))\n",
    "print('Spec umbral 0.3 =', specificy(y_test, y_pred_logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Acc umbral 0.5 =', accuracy_score(y_test, y_pred_orig))\n",
    "print('Acc umbral 0.3 =', accuracy_score(y_test, y_pred_logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('F1 score 0.5 =', f1_score(y_test, y_pred_orig))\n",
    "print('F1 score 0.3 =', f1_score(y_test, y_pred_logit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se puede ajustar el umbral para las predicciones en clasificadores binarios\n",
    "* El ajuste de este umbral repercute sobre las diferentes medidas de performance.\n",
    "* Particularmente, sensitivity y specificity tiene una relación inversa\n",
    "    * siempre al mejorar uno, empeorará el otro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Curvas ROC y área bajo la curva (AUC)\n",
    "\n",
    "Muy útil si queremos visualizar cómo se mueven sensitivity y specificity ante diversos umbrales. \n",
    "\n",
    "La curva ROC se basa en $TPR$ (tasa de verdaderos positivos) y $FPR$ (tasa de falsos negativos).\n",
    "\n",
    "* Definamos las metricas de True Positive Ratio y False Positive Rate y se las asignamos las los valores predichos vs los valores de test (observados vs esperados).\n",
    "* El método `roc_curve` toma como parámetros dos valores: los valores observados del target y un array de probabilidades (NO recibe las predicciones de la clase).\n",
    "* Devuelve tres elementos en forma de arrays: la tasa de falsos positivos ($FPR$), la tasa de verdaderos positivos ($TPR$) y los umbrales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_log,tpr_log,thr_log = roc_curve(y_test, y_probs_logit[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convertimos los valores en un objeto dataframe y graficamos la curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(fpr=fpr_log, tpr=tpr_log, thr = thr_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis([0, 1.01, 0, 1.01])\n",
    "plt.xlabel('FPR = 1 - Specificty')\n",
    "plt.ylabel('TPR = Sensitivity = Recall')\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(df['fpr'],df['tpr'])\n",
    "plt.plot(np.arange(0,1, step =0.01), np.arange(0,1, step =0.01))\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculamos el área bajo la curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC=', auc(fpr_log, tpr_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PRÁCTICA INDEPENDIENTE\n",
    "\n",
    "Evaluar las medidas vistas en ambas prácticas para un clasificador Naïve Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model_nb = GaussianNB()\n",
    "model_nb.fit(X_train, y_train)\n",
    "y_fit_nb = model_nb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_log,tpr_log,thr_log = roc_curve(y_test, y_fit_nb[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(fpr=fpr_log, tpr=tpr_log, thr = thr_log))\n",
    "\n",
    "plt.axis([0, 1.01, 0, 1.01])\n",
    "plt.xlabel('FPR = 1 - Specificty')\n",
    "plt.ylabel('TPR = Sensitivity = Recall')\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(df['fpr'],df['tpr'])\n",
    "plt.plot(np.arange(0,1, step =0.01), np.arange(0,1, step =0.01))\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC=', auc(fpr_log, tpr_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(1,20,1):\n",
    "    model = KNeighborsClassifier(n_neighbors=i) \n",
    "    cv_scores = cross_val_score(model, X_test, y_test, cv=5)\n",
    "\n",
    "    scores.append({\n",
    "        'score_mean': np.mean(cv_scores),\n",
    "        'score_std': np.std(cv_scores),\n",
    "        'n': i\n",
    "    })\n",
    "\n",
    "scores_knn = pd.DataFrame(scores)\n",
    "scores_knn['low'] = scores_knn['score_mean'] - scores_knn['score_std']\n",
    "scores_knn['high'] = scores_knn['score_mean'] + scores_knn['score_std']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(scores_knn['n'], scores_knn['low'], color='r')\n",
    "plt.plot(scores_knn['n'], scores_knn['score_mean'], color='b')\n",
    "plt.plot(scores_knn['n'], scores_knn['high'], color='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_fit_knn = knn_model.predict_proba(X_test)\n",
    "fpr_log,tpr_log,thr_log = roc_curve(y_test, y_fit_knn[:,1])\n",
    "\n",
    "df = pd.DataFrame(dict(fpr=fpr_log, tpr=tpr_log, thr = thr_log))\n",
    "\n",
    "plt.axis([0, 1.01, 0, 1.01])\n",
    "plt.xlabel('FPR = 1 - Specificty')\n",
    "plt.ylabel('TPR = Sensitivity = Recall')\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(df['fpr'],df['tpr'])\n",
    "plt.plot(np.arange(0,1, step =0.01), np.arange(0,1, step =0.01))\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC=', auc(fpr_log, tpr_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit_knn = knn_model.predict(X_test)\n",
    "confusion_matrix(y_test,y_fit_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model_svc = SVC(probability=True)\n",
    "model_svc.fit(X_train, y_train)\n",
    "y_fit_svc = model_svc.predict_proba(X_test)\n",
    "\n",
    "fpr_log,tpr_log,thr_log = roc_curve(y_test, y_fit_svc[:,1])\n",
    "\n",
    "df = pd.DataFrame(dict(fpr=fpr_log, tpr=tpr_log, thr = thr_log))\n",
    "\n",
    "plt.axis([0, 1.01, 0, 1.01])\n",
    "plt.xlabel('FPR = 1 - Specificty')\n",
    "plt.ylabel('TPR = Sensitivity = Recall')\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(df['fpr'],df['tpr'])\n",
    "plt.plot(np.arange(0,1, step =0.01), np.arange(0,1, step =0.01))\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC=', auc(fpr_log, tpr_log))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
