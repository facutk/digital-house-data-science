{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA GUIADA: Pipelines StumbleUpon Evergreen\n",
    "\n",
    "## 1. Introducción\n",
    "\n",
    "Utilizaremos el dataset de StambleUpon para armar nuestro primer Pipeline. StambleUpon es un sitiuo web que recomienda páginas y contenido a sus usuarios basados en los intereses de estos últimos. Entre esas páginas recomendadas hay algunas que tienen períodos cortos de relevancia (noticias, recetas de cocina, etc.) y hay otras que matienen interés a lo largo del tiempo y pueden ser recomendadas a los usuarios mucho tiempo después de que han sido publicadas. Las páginas pueden ser clasificadas en \"ephemeral\" (efímeras) o \"evergreen\" (perennes).\n",
    "\n",
    "El objetivo es, entonces, poder construir un clasificador que clasifique las páginas en estas dos categorías para poder mejorar el sistema de recomendación del sitio.\n",
    "\n",
    "Para ello, trataremos de mostrar la utilidad que tiene los pipelines.\n",
    "\n",
    "**Nota:** esta práctica está basada en un [desafío de Kaggle](https://www.kaggle.com/c/stumbleupon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipelines \"simples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Primero importaremos los datos, paquetes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {\"title\":\"IBM Sees Holographic Calls Air Breat...\n",
       "1    {\"title\":\"The Fully Electronic Futuristic Star...\n",
       "2    {\"title\":\"Fruits that Fight the Flu fruits tha...\n",
       "3    {\"title\":\"10 Foolproof Tips for Better Sleep \"...\n",
       "4    {\"title\":\"The 50 Coolest Jerseys You Didn t Kn...\n",
       "Name: boilerplate, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"../Data/stumbleupon.tsv\", sep='\\t')\n",
    "data['boilerplate'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tomamos del campo “boilerplate” los subcampos “title” y “body” y los agregamos a data\n",
    "* Rellenamos vacíos con ''\n",
    "\n",
    "* Verificamos los valores obtenidos en el vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>urlid</th>\n",
       "      <th>boilerplate</th>\n",
       "      <th>alchemy_category</th>\n",
       "      <th>alchemy_category_score</th>\n",
       "      <th>avglinksize</th>\n",
       "      <th>commonlinkratio_1</th>\n",
       "      <th>commonlinkratio_2</th>\n",
       "      <th>commonlinkratio_3</th>\n",
       "      <th>commonlinkratio_4</th>\n",
       "      <th>...</th>\n",
       "      <th>linkwordscore</th>\n",
       "      <th>news_front_page</th>\n",
       "      <th>non_markup_alphanum_characters</th>\n",
       "      <th>numberOfLinks</th>\n",
       "      <th>numwords_in_url</th>\n",
       "      <th>parametrizedLinkRatio</th>\n",
       "      <th>spelling_errors_ratio</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bloomberg.com/news/2010-12-23/ibm-p...</td>\n",
       "      <td>4042</td>\n",
       "      <td>{\"title\":\"IBM Sees Holographic Calls Air Breat...</td>\n",
       "      <td>business</td>\n",
       "      <td>0.789131</td>\n",
       "      <td>2.055556</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>5424</td>\n",
       "      <td>170</td>\n",
       "      <td>8</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.079130</td>\n",
       "      <td>0</td>\n",
       "      <td>IBM Sees Holographic Calls Air Breathing Batte...</td>\n",
       "      <td>A sign stands outside the International Busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.popsci.com/technology/article/2012-...</td>\n",
       "      <td>8471</td>\n",
       "      <td>{\"title\":\"The Fully Electronic Futuristic Star...</td>\n",
       "      <td>recreation</td>\n",
       "      <td>0.574147</td>\n",
       "      <td>3.677966</td>\n",
       "      <td>0.508021</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>0.213904</td>\n",
       "      <td>0.144385</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4973</td>\n",
       "      <td>187</td>\n",
       "      <td>9</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.125448</td>\n",
       "      <td>1</td>\n",
       "      <td>The Fully Electronic Futuristic Starting Gun T...</td>\n",
       "      <td>And that can be carried on a plane without the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.menshealth.com/health/flu-fighting-...</td>\n",
       "      <td>1164</td>\n",
       "      <td>{\"title\":\"Fruits that Fight the Flu fruits tha...</td>\n",
       "      <td>health</td>\n",
       "      <td>0.996526</td>\n",
       "      <td>2.382883</td>\n",
       "      <td>0.562016</td>\n",
       "      <td>0.321705</td>\n",
       "      <td>0.120155</td>\n",
       "      <td>0.042636</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>2240</td>\n",
       "      <td>258</td>\n",
       "      <td>11</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>1</td>\n",
       "      <td>Fruits that Fight the Flu fruits that fight th...</td>\n",
       "      <td>Apples The most popular source of antioxidants...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.dumblittleman.com/2007/12/10-foolpr...</td>\n",
       "      <td>6684</td>\n",
       "      <td>{\"title\":\"10 Foolproof Tips for Better Sleep \"...</td>\n",
       "      <td>health</td>\n",
       "      <td>0.801248</td>\n",
       "      <td>1.543103</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2737</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.100858</td>\n",
       "      <td>1</td>\n",
       "      <td>10 Foolproof Tips for Better Sleep</td>\n",
       "      <td>There was a period in my life when I had a lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://bleacherreport.com/articles/1205138-the...</td>\n",
       "      <td>9006</td>\n",
       "      <td>{\"title\":\"The 50 Coolest Jerseys You Didn t Kn...</td>\n",
       "      <td>sports</td>\n",
       "      <td>0.719157</td>\n",
       "      <td>2.676471</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.043210</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>12032</td>\n",
       "      <td>162</td>\n",
       "      <td>10</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.082569</td>\n",
       "      <td>0</td>\n",
       "      <td>The 50 Coolest Jerseys You Didn t Know Existed...</td>\n",
       "      <td>Jersey sales is a curious business Whether you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  urlid  \\\n",
       "0  http://www.bloomberg.com/news/2010-12-23/ibm-p...   4042   \n",
       "1  http://www.popsci.com/technology/article/2012-...   8471   \n",
       "2  http://www.menshealth.com/health/flu-fighting-...   1164   \n",
       "3  http://www.dumblittleman.com/2007/12/10-foolpr...   6684   \n",
       "4  http://bleacherreport.com/articles/1205138-the...   9006   \n",
       "\n",
       "                                         boilerplate alchemy_category  \\\n",
       "0  {\"title\":\"IBM Sees Holographic Calls Air Breat...         business   \n",
       "1  {\"title\":\"The Fully Electronic Futuristic Star...       recreation   \n",
       "2  {\"title\":\"Fruits that Fight the Flu fruits tha...           health   \n",
       "3  {\"title\":\"10 Foolproof Tips for Better Sleep \"...           health   \n",
       "4  {\"title\":\"The 50 Coolest Jerseys You Didn t Kn...           sports   \n",
       "\n",
       "  alchemy_category_score  avglinksize  commonlinkratio_1  commonlinkratio_2  \\\n",
       "0               0.789131     2.055556           0.676471           0.205882   \n",
       "1               0.574147     3.677966           0.508021           0.288770   \n",
       "2               0.996526     2.382883           0.562016           0.321705   \n",
       "3               0.801248     1.543103           0.400000           0.100000   \n",
       "4               0.719157     2.676471           0.500000           0.222222   \n",
       "\n",
       "   commonlinkratio_3  commonlinkratio_4  \\\n",
       "0           0.047059           0.023529   \n",
       "1           0.213904           0.144385   \n",
       "2           0.120155           0.042636   \n",
       "3           0.016667           0.000000   \n",
       "4           0.123457           0.043210   \n",
       "\n",
       "                         ...                          linkwordscore  \\\n",
       "0                        ...                                     24   \n",
       "1                        ...                                     40   \n",
       "2                        ...                                     55   \n",
       "3                        ...                                     24   \n",
       "4                        ...                                     14   \n",
       "\n",
       "   news_front_page  non_markup_alphanum_characters  numberOfLinks  \\\n",
       "0                0                            5424            170   \n",
       "1                0                            4973            187   \n",
       "2                0                            2240            258   \n",
       "3                0                            2737            120   \n",
       "4                0                           12032            162   \n",
       "\n",
       "   numwords_in_url  parametrizedLinkRatio  spelling_errors_ratio label  \\\n",
       "0                8               0.152941               0.079130     0   \n",
       "1                9               0.181818               0.125448     1   \n",
       "2               11               0.166667               0.057613     1   \n",
       "3                5               0.041667               0.100858     1   \n",
       "4               10               0.098765               0.082569     0   \n",
       "\n",
       "                                               title  \\\n",
       "0  IBM Sees Holographic Calls Air Breathing Batte...   \n",
       "1  The Fully Electronic Futuristic Starting Gun T...   \n",
       "2  Fruits that Fight the Flu fruits that fight th...   \n",
       "3                10 Foolproof Tips for Better Sleep    \n",
       "4  The 50 Coolest Jerseys You Didn t Know Existed...   \n",
       "\n",
       "                                                body  \n",
       "0  A sign stands outside the International Busine...  \n",
       "1  And that can be carried on a plane without the...  \n",
       "2  Apples The most popular source of antioxidants...  \n",
       "3  There was a period in my life when I had a lot...  \n",
       "4  Jersey sales is a curious business Whether you...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'] = data.boilerplate.apply(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.apply(lambda x: json.loads(x).get('body', ''))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    IBM Sees Holographic Calls Air Breathing Batte...\n",
       "1    The Fully Electronic Futuristic Starting Gun T...\n",
       "2    Fruits that Fight the Flu fruits that fight th...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = data['title'].fillna('')\n",
    "body =  data['body'].fillna('')\n",
    "y = data['label']\n",
    "titles[0:3]\n",
    "\n",
    "#y[0:3]\n",
    "#y.value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanceo de la clase\n",
    "\n",
    "Verifiquemos cómo se encuentra balanceada la clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3796\n",
       "0    3599\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase parece bien balanceada, por lo tanto el accuracy será una buena medida de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentación\n",
    "\n",
    "Antes de encarar la construcción de un pipeline tenemos que determinar qué posibles combinaciones de preprocesamiento y modelos vamos a explorar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En el preprocesamiento, vamos a usar la clase `CountVectorizer` para extraer a partir de los títulos, un vector de palabras.\n",
    "\n",
    "    **Parámetros:**\n",
    "\n",
    "    1. `max_features`: Sólo considera las primeras X características, ordenadas por frecuencia.\n",
    "    2. `ngram_range` : tuple (min_n, max_n): Va a tomar palabras de a una y de a dos.\n",
    "    3. `stop_words`: Va a descartar artículos y palabras sin poder predictivo del idioma inglés. Se pueden usar listas custom.\n",
    "    4. `binary`: Las posibilidades son 0 o 1(no acumula).\n",
    "    \n",
    "    \n",
    "* Para el modelo de clasificación, por ser basado en texto vamos a utilizar MultinomialNB que no tiene hiperparámetros para explorar. \n",
    "\n",
    "\n",
    "Con estos pasos vamos a crear un pipeline que contenga:\n",
    "\n",
    "     1. El vectorizador de texto\n",
    "     2. El modelo de regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/test\n",
    "\n",
    "Para tener una estimación de la performance del modelo seleccionado sobre datos no observados, comenzamos por hacer un split train/test sobre los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[['title','body']].fillna('')\n",
    "y = data['label']\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline simple sin Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos y creamos el pipeline\n",
    "Lo entrenamos con el set de entrenamiento y lo ejecutamos sobre el testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "model = MultinomialNB()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vec', vectorizer),\n",
    "        ('model', model)   \n",
    "    ])\n",
    "\n",
    "# Vamos a ejecutar el pipeline sobre los títulos\n",
    "X_train_tit = X_train['title']\n",
    "X_test_tit = X_test['title']\n",
    "\n",
    "pipeline.fit(X_train_tit, y_train)\n",
    "pred = pipeline.predict(X_test_tit)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparemos la predicción con el label\n",
    "* Para eso, pasamos el array de predicciones a un boolean para comparar con los labels y ejecutamos el reporte de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.72      0.75      1198\n",
      "          1       0.75      0.81      0.78      1243\n",
      "\n",
      "avg / total       0.77      0.77      0.77      2441\n",
      "\n",
      "0.766898811962\n"
     ]
    }
   ],
   "source": [
    "#pred_bool=pred[:,0]<0.5\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(classification_report(y_test, pred))\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combinando pipelines y GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora como utilizar conjuntamente los pipelines junto con el tunning de hiperparámetros con `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos un pipeline que tiene tres etapas:\n",
    "\n",
    "1. Un vectorizador de texto: `CountVectorizer`\n",
    "2. Un transformador de la matriz original `TfidfTransformer`\n",
    "3. Un clasificador basado en Multinomial Naive Bayes\n",
    "\n",
    "Notar que en este caso no los instanciamos previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "   ('vect', CountVectorizer()), \n",
    "   ('tfidf', TfidfTransformer()), \n",
    "   ('clf', MultinomialNB()), \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimentación\n",
    "\n",
    "* Definimos los parámetros a buscar.\n",
    "  - Es importante notar la forma en que se pasan los parámetros: en general, se escriben `[nombre de la etapa]__[parametro]`.\n",
    "  En esta primera etapa queremos determinar si es beneficioso agregar al modelo nuevos n-gramas (combinaciones de dos palabras) como features y si tenemos que eliminar palabras que figuran menos de determinada cantidad de veces en el corpus\n",
    "* Entonces, los parámetros que usamos en el `GridSeachCV` son \n",
    "  - para `CountVectorizer` (llamado `vect` en el pipeline): `min_df` y `n_gram_range`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__min_df': [1,2,3,4],\n",
    "    'vect__max_df': np.linspace(0.01,1,5),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV (pipeline, parameters, n_jobs = 3 , verbose = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n",
      "[CV] vect__max_df=0.01, vect__min_df=1, vect__ngram_range=(1, 1) .....\n",
      "[CV] vect__max_df=0.01, vect__min_df=1, vect__ngram_range=(1, 1) .....\n",
      "[CV] vect__max_df=0.01, vect__min_df=1, vect__ngram_range=(1, 1) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.01, vect__min_df=1, vect__ngram_range=(1, 2) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=0.01, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.01, vect__min_df=1, vect__ngram_range=(1, 2) .....\n",
      "[CV] vect__max_df=0.01, vect__min_df=1, vect__ngram_range=(1, 2) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.01, vect__min_df=2, vect__ngram_range=(1, 1) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.01, vect__min_df=2, vect__ngram_range=(1, 1) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.01, vect__min_df=2, vect__ngram_range=(1, 1) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.01, vect__min_df=2, vect__ngram_range=(1, 2) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=0.01, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.01, vect__min_df=2, vect__ngram_range=(1, 2) .....\n",
      "[CV] vect__max_df=0.01, vect__min_df=2, vect__ngram_range=(1, 2) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.01, vect__min_df=3, vect__ngram_range=(1, 1) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.01, vect__min_df=3, vect__ngram_range=(1, 1) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.01, vect__min_df=3, vect__ngram_range=(1, 1) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.01, vect__min_df=3, vect__ngram_range=(1, 2) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.01, vect__min_df=3, vect__ngram_range=(1, 2) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.01, vect__min_df=3, vect__ngram_range=(1, 2) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.01, vect__min_df=4, vect__ngram_range=(1, 1) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.01, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.01, vect__min_df=4, vect__ngram_range=(1, 1) .....\n",
      "[CV] vect__max_df=0.01, vect__min_df=4, vect__ngram_range=(1, 1) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.01, vect__min_df=4, vect__ngram_range=(1, 2) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=0.01, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.01, vect__min_df=4, vect__ngram_range=(1, 2) .....\n",
      "[CV] vect__max_df=0.01, vect__min_df=4, vect__ngram_range=(1, 2) .....\n",
      "[CV]  vect__max_df=0.01, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=1, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.01, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.01, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=1, vect__ngram_range=(1, 1) ...\n",
      "[CV] vect__max_df=0.2575, vect__min_df=1, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=1, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=1, vect__ngram_range=(1, 2) ...\n",
      "[CV] vect__max_df=0.2575, vect__min_df=1, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=2, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=2, vect__ngram_range=(1, 1) ...\n",
      "[CV] vect__max_df=0.2575, vect__min_df=2, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=2, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=2, vect__ngram_range=(1, 2) ...\n",
      "[CV] vect__max_df=0.2575, vect__min_df=2, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=3, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=3, vect__ngram_range=(1, 1) ...\n",
      "[CV] vect__max_df=0.2575, vect__min_df=3, vect__ngram_range=(1, 1) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    8.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__max_df=0.2575, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=3, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=3, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=3, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=4, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=4, vect__ngram_range=(1, 1) ...\n",
      "[CV] vect__max_df=0.2575, vect__min_df=4, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=4, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.2575, vect__min_df=4, vect__ngram_range=(1, 2) ...\n",
      "[CV] vect__max_df=0.2575, vect__min_df=4, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.505, vect__min_df=1, vect__ngram_range=(1, 1) ....\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.2575, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.505, vect__min_df=1, vect__ngram_range=(1, 1) ....\n",
      "[CV] vect__max_df=0.505, vect__min_df=1, vect__ngram_range=(1, 1) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.505, vect__min_df=1, vect__ngram_range=(1, 2) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=0.505, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.505, vect__min_df=1, vect__ngram_range=(1, 2) ....\n",
      "[CV] vect__max_df=0.505, vect__min_df=1, vect__ngram_range=(1, 2) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.505, vect__min_df=2, vect__ngram_range=(1, 1) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.505, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.505, vect__min_df=2, vect__ngram_range=(1, 1) ....\n",
      "[CV] vect__max_df=0.505, vect__min_df=2, vect__ngram_range=(1, 1) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.505, vect__min_df=2, vect__ngram_range=(1, 2) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=0.505, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.505, vect__min_df=2, vect__ngram_range=(1, 2) ....\n",
      "[CV] vect__max_df=0.505, vect__min_df=2, vect__ngram_range=(1, 2) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.505, vect__min_df=3, vect__ngram_range=(1, 1) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.505, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.505, vect__min_df=3, vect__ngram_range=(1, 1) ....\n",
      "[CV] vect__max_df=0.505, vect__min_df=3, vect__ngram_range=(1, 1) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.505, vect__min_df=3, vect__ngram_range=(1, 2) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=0.505, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.505, vect__min_df=3, vect__ngram_range=(1, 2) ....\n",
      "[CV] vect__max_df=0.505, vect__min_df=3, vect__ngram_range=(1, 2) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.505, vect__min_df=4, vect__ngram_range=(1, 1) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.505, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.505, vect__min_df=4, vect__ngram_range=(1, 1) ....\n",
      "[CV] vect__max_df=0.505, vect__min_df=4, vect__ngram_range=(1, 1) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.505, vect__min_df=4, vect__ngram_range=(1, 2) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.505, vect__min_df=4, vect__ngram_range=(1, 2) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.505, vect__min_df=4, vect__ngram_range=(1, 2) ....\n",
      "[CV]  vect__max_df=0.505, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=1, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.505, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=1, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.505, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=1, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=1, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=1, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=1, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=2, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=2, vect__ngram_range=(1, 1) ...\n",
      "[CV] vect__max_df=0.7525, vect__min_df=2, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=2, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=2, vect__ngram_range=(1, 2) ...\n",
      "[CV] vect__max_df=0.7525, vect__min_df=2, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=3, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=3, vect__ngram_range=(1, 1) ...\n",
      "[CV] vect__max_df=0.7525, vect__min_df=3, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=3, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=3, vect__ngram_range=(1, 2) ...\n",
      "[CV] vect__max_df=0.7525, vect__min_df=3, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=4, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=4, vect__ngram_range=(1, 1) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] vect__max_df=0.7525, vect__min_df=4, vect__ngram_range=(1, 1) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=4, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=0.7525, vect__min_df=4, vect__ngram_range=(1, 2) ...\n",
      "[CV] vect__max_df=0.7525, vect__min_df=4, vect__ngram_range=(1, 2) ...\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=1.0, vect__min_df=1, vect__ngram_range=(1, 1) ......\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=0.7525, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=1.0, vect__min_df=1, vect__ngram_range=(1, 1) ......\n",
      "[CV] vect__max_df=1.0, vect__min_df=1, vect__ngram_range=(1, 1) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=1.0, vect__min_df=1, vect__ngram_range=(1, 2) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=1.0, vect__min_df=1, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=1.0, vect__min_df=1, vect__ngram_range=(1, 2) ......\n",
      "[CV] vect__max_df=1.0, vect__min_df=1, vect__ngram_range=(1, 2) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=1.0, vect__min_df=2, vect__ngram_range=(1, 1) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=1.0, vect__min_df=1, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=1.0, vect__min_df=2, vect__ngram_range=(1, 1) ......\n",
      "[CV] vect__max_df=1.0, vect__min_df=2, vect__ngram_range=(1, 1) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=1.0, vect__min_df=2, vect__ngram_range=(1, 2) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=1.0, vect__min_df=2, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=1.0, vect__min_df=2, vect__ngram_range=(1, 2) ......\n",
      "[CV] vect__max_df=1.0, vect__min_df=2, vect__ngram_range=(1, 2) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=1.0, vect__min_df=3, vect__ngram_range=(1, 1) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=1.0, vect__min_df=2, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=1.0, vect__min_df=3, vect__ngram_range=(1, 1) ......\n",
      "[CV] vect__max_df=1.0, vect__min_df=3, vect__ngram_range=(1, 1) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=1.0, vect__min_df=3, vect__ngram_range=(1, 2) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV]  vect__max_df=1.0, vect__min_df=3, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=1.0, vect__min_df=3, vect__ngram_range=(1, 2) ......\n",
      "[CV] vect__max_df=1.0, vect__min_df=3, vect__ngram_range=(1, 2) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=1.0, vect__min_df=4, vect__ngram_range=(1, 1) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=1.0, vect__min_df=4, vect__ngram_range=(1, 1) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=3, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV] vect__max_df=1.0, vect__min_df=4, vect__ngram_range=(1, 1) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=1.0, vect__min_df=4, vect__ngram_range=(1, 2) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=1.0, vect__min_df=4, vect__ngram_range=(1, 2) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=4, vect__ngram_range=(1, 1), total=   0.3s\n",
      "[CV] vect__max_df=1.0, vect__min_df=4, vect__ngram_range=(1, 2) ......\n",
      "[CV]  vect__max_df=1.0, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=1.0, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n",
      "[CV]  vect__max_df=1.0, vect__min_df=4, vect__ngram_range=(1, 2), total=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 120 out of 120 | elapsed:   27.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=3,\n",
       "       param_grid={'vect__min_df': [1, 2, 3, 4], 'vect__max_df': array([ 0.01  ,  0.2575,  0.505 ,  0.7525,  1.    ]), 'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Performing grid search...\") \n",
    "grid_search.fit(X_train_tit, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* E imprimimos los mejores parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.756\n",
      "Best parameters set:\n",
      "\t vect__max_df: 0.25750000000000001\n",
      "\t vect__min_df: 2\n",
      "\t vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search . best_score_) \n",
    "print(\"Best parameters set:\" )\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted (parameters . keys()): \n",
    "                    print(\"\\t %s: %r\" % (param_name, best_parameters[param_name])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluando la performance de la búsqueda sobre datos no observados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.fit(X_train_tit,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test_tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** BONUS:** ¿Qué tanto mejor es el tiempo de cómputo con `RandomizedSearchCV`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rand_search = RandomizedSearchCV(pipeline, parameters, n_jobs = 3 , verbose = 2, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Performing randomized search...\") \n",
    "rand_search.fit(X_train_tit, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipelines y Gridsearch con funciones propias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces las clases que existen en el módulo de preprocesamiento de sklearn pueden \"quedarnos chicas\". Es decir, puede ser que tengamos que definir alguna otra transformación para el preprocesamiento que no exista en el módulo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Extender la BaseClass en Scikit-Learn. \n",
    "\n",
    "\n",
    "En este ejemplo creamos un transformador muy simple que devuelve la entrada multiplicada por un factor X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BodyIncluder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,include_body=False):\n",
    "        self.include_body = include_body\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if (self.include_body):\n",
    "            \n",
    "            return X['title'].astype(str) + X['body'].astype(str)\n",
    "        else:\n",
    "            return X['title']\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bi = BodyIncluder(include_body= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bi.transform(X_train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bi = BodyIncluder(include_body=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bi.transform(X_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Supongamos que quisiéramos generar un transformador que extrajera el largo del cuerpo de los textos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Experimentando en el pipeline con Body Includer\n",
    "\n",
    "Queremos probar aumentar la complejidad del modelo incluyendo el cuerpo de las páginas y no únicamente el título."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "   ('bi', BodyIncluder()),  \n",
    "   ('vect', CountVectorizer()), \n",
    "   ('tfidf', TfidfTransformer()), \n",
    "   ('clf', MultinomialNB()), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__min_df': [2,3,4],\n",
    "    #'vect__max_df': np.linspace(0.01,1,5),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'vect__stop_words': ['english',None],\n",
    "    'bi__include_body': [True,False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV (pipeline, parameters, n_jobs = 3 , verbose = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Performing grid search...\") \n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search . best_score_) \n",
    "print(\"Best parameters set:\" )\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted (parameters . keys()): \n",
    "                    print(\"\\t %s: %r\" % (param_name, best_parameters[param_name])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Evaluamos el modelo sobre datos no observados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Usando la función  FunctionTransformer del módulo de pre-procesamiento\n",
    "\n",
    "FunctionTransformer es otra manera de generar features con transformaciones definidas por el usuario.\n",
    "\n",
    "* Si queremos generar un paso que aplique transformaciones matemáticas puntuales a los features podemos utilizar FunctionTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = FunctionTransformer(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0.5,1],[2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer.transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
