{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica Independiente: El clasificador Bagging en Scikit Learn\n",
    "\n",
    "En scikit-learn, los métodos de bagging se ofrecen como un meta-estimador unificado de `BaggingClassifier` (respectivamente ` BaggingRegressor`), tomando como entrada un estimador de base definido por el usuario junto con parámetros que especifican la estrategia para construir subsets aleatorios.    \n",
    "En particular, `max_samples` y ` max_features` controlan el tamaño de los subsets (en términos de muestras y características), mientras que `bootstrap` y ` bootstrap_features` controlan si las muestras y características se toman con o sin reemplazo.   \n",
    "Cuando se utiliza un subset de las muestras disponibles, el error de generalización se puede estimar con las muestras fuera de bolsa (out-of-bag) poniendo `oob_score=True`.\n",
    "\n",
    "Como ejemplo, vamos a comparar el rendimiento de un clasificador KNN simple versus el clasificador de Bagging en el conjunto de datos de aceptabilidad de autos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es leer los datos en Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('car.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento\n",
    "\n",
    "Cada una de las catgorías parece ser un nivel de la variable. Exploren la composición de cada serie con el método value_counts() ¿Cómo parecen haber sido generadas las categorías?\n",
    "\n",
    "#### Escalar las variables\n",
    "\n",
    "A continuación transformemos las categorías ordinales en niveles escalares para las variables buying, maint, lug_boot y safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.buying = df.buying.map({'low':1,'med':2,'high':3,'vhigh':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.maint = df.maint.map({'low':1,'med':2,'high':3,'vhigh':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.lug_boot = df.lug_boot.map({'small':1,'med':2,'big':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.safety = df.safety.map({'low':1,'med':2,'high':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcular dummies\n",
    "\n",
    "Para las variables doors y persons calculamos dummis porque hay niveles que no se pueden recodificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dum = pd.get_dummies(df[['doors','persons']],drop_first =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([df_dum,df[['buying','maint','lug_boot','safety']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación binaria\n",
    "\n",
    "Para simplificar, transformemos el problema en uno de clasificación binaria recodificando \"unnacceptable\" como 0 y todos los demás estados como 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df.acceptability.map({'unacc':0,'acc':1,'good':1,'vgood':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de ensambles\n",
    "\n",
    "El siguiente paso es calcular el cross_val_score en los dos clasificadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "my_cv = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "bagging = BaggingClassifier(knn, random_state=1)\n",
    "\n",
    "print (\"KNN Score:\\t\", cross_val_score(knn, X, y, cv=my_cv, n_jobs=-1).mean())\n",
    "print (\"Bagging Score:\\t\", cross_val_score(bagging, X, y, cv=my_cv, n_jobs=-1).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
