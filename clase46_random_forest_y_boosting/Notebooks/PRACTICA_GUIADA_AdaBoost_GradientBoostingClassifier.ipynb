{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA GUIADA 2: Clasificadores Ada Boost y Gradient Boosting\n",
    "\n",
    "\n",
    "## 1. Introducción\n",
    "\n",
    "El objetivo de esta práctica es comenzar a aproximarnos al uso y a la evaluación de los métodos algoritmos de Boosting.\n",
    "\n",
    "Recordemos: \n",
    "\n",
    "* La idea fundamental de los algoritmos basados en la noción de boosting era tratar de combinar muchos clasificadores débiles en un clasificador fuerte. \n",
    "\n",
    "* La principal diferencia con los métodos de Bagging era la forma en que se construye el ensamble: Boosting trata de mejorar la performance concentrándose de alguna forma en los casos con mayor error de entrenamiento.\n",
    "\n",
    "### `AdaBoostClassifier()`\n",
    "\n",
    "La idea central de AdaBoost es construir un ensamble de week learners y, en cada iteración ir incrementando el peso de los casos mal clasificados. La implementación den Scikit-Learn toma los siguientes parámetros:\n",
    "\n",
    "* `base_estimator`: análogo al caso de `BaggingClassifier()`, el estimador sobre el cual se va a construir el ensamble. Por efecto, son árboles de decisión.\n",
    "* `n_estimators`: el máximo de iteraciones\n",
    "* `learning_rate`: el peso que va a tener la predicción de cada árbol en el ensamble final\n",
    "\n",
    "\n",
    "### `GradientBoostingClassifier()`\n",
    "\n",
    "Se trata de una generalización del algoritmo general de Boosting para cualquier tipo de función de pérdida diferenciable. En cada etapa, se fitea un árbol de decisión pero se realiza sobre los residuos del árbol anterior. Es decir, se busca corregir las estimaciones entrenando nuevos clasificadores sobre los \"residuos\" (la diferencia entre el valor observado y el valor predico ($y - \\hat{y}$)\n",
    "\n",
    "Los argumentos que toma como input son ya conocidos:\n",
    "\n",
    "* `learning_rate`: el peso que va a tener la predicción de cada árbol en el ensamble final\n",
    "\n",
    "* `n_estimators`: el máximo de iteraciones\n",
    "* `criterion`: define el criterio de impureza para evaluar la calidad de las particiones\n",
    "* `max_features`: la cantidad de features que extraerá para entrenar cada `base_estimator`. Por default es igual a `sqrt(X.shape[1])`\n",
    "* `bootstrap` y `bootstrap_features`: controla si tanto los n_samples como las features son extraidos con reposición.\n",
    "* `max_depth`: la pronfundidad máxima del árbol\n",
    "* `min_samples_leaf`: el número mínimo de n_samples para constituir una hoja del árbol (nodo terminal)\n",
    "* `min_samples_split`: el número mínimo de n_samples para realizar un split.\n",
    "\n",
    "Utilizaremos como punto de partida el mismo código que utilizamos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../Data/car.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = LabelEncoder().fit_transform(df['acceptability'])\n",
    "X = pd.get_dummies(df.drop('acceptability', axis=1), drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=3, random_state=41, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a inicializar el clasificador de árbol de decisión y evaluar su rendimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento de Árbol de decisión:\t0.898 ± 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def evaluar_rendimiento(modelo, nombre):\n",
    "    s = cross_val_score(modelo, X, y, cv=cv, n_jobs=-1)\n",
    "    print(\"Rendimiento de {}:\\t{:0.3} ± {:0.3}\".format( \\\n",
    "        nombre, s.mean().round(3), s.std().round(3)))\n",
    "    \n",
    "    \n",
    "dt = DecisionTreeClassifier(class_weight='balanced')\n",
    "\n",
    "evaluar_rendimiento(dt,\"Árbol de decisión\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ustedes evalúen los siguientes clasificadores:\n",
    " * AdaBoostClassifier\n",
    " * GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento de AdaBoostClassifier:\t0.845 ± 0.01\n",
      "Rendimiento de GradientBoostingClassifier:\t0.955 ± 0.011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "ab = AdaBoostClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "evaluar_rendimiento(ab, \"AdaBoostClassifier\")\n",
    "evaluar_rendimiento(gb, \"GradientBoostingClassifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Puede verse, entonces, que AdaBoost performa bastante peor (al menos utilizando los parámetros por defecto). De esta forma, podríamos tratar de tunear los hiperparámetros para hacerlo funcionar mejor. Haremos esto en el LAB con otro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
